{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa212fd2-408b-4e95-b543-819e612cbeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Parsed portfolio weights:\n",
      "\n",
      "SGOV = 38.38%\n",
      "ULTY = 15.87%\n",
      "RDDT = 14.69%\n",
      "GOOGL = 9.79%\n",
      "META = 9.05%\n",
      "ASTS = 7.34%\n",
      "QQQM = 4.87%\n",
      "BRK/B = 0.00%\n",
      "HOOD = 0.00%\n",
      "INTU = 0.00%\n",
      "NBIS = 0.00%\n",
      "SHOP = 0.00%\n",
      "SNOW = 0.00%\n",
      "TQQQ = 0.00%\n",
      "\n",
      "✅ Total portfolio weight: 100.00%\n",
      "✅ Portfolio weights loaded and validated successfully.\n",
      "Wrote: csv_export\\portfolio_weights.csv\n",
      "Risk Dashboard - 5 years (1250 days)\n",
      "Annualized Return      : 17.48%\n",
      "Annualized Volatility  : 21.71%\n",
      "Sharpe Ratio (rf=5%)   : 0.57\n",
      "Max Drawdown           : -33.01%\n",
      "1-Day VaR  (95%): -2.04% (~$-2,037)\n",
      "1-Day CVaR (95%): -2.97% (~$-2,968)\n",
      "Beta vs. S&P 500       : 0.90\n",
      "Beta vs. Nasdaq-100    : 0.72\n",
      "❤️ DAISY ❤️\n",
      "✅ Wrote: csv_export\\Returns.csv\n",
      "✅ Wrote: csv_export\\RDDT.csv\n",
      "✅ Wrote: csv_export\\HOOD.csv\n",
      "✅ Wrote: csv_export\\NBIS.csv\n",
      "✅ Wrote: csv_export\\SNOW.csv\n",
      "✅ Wrote: csv_export\\SHOP.csv\n",
      "\n",
      "📁 All exports saved in: C:\\Users\\ahv25\\Desktop\\Python\\csv_export\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Daisy Portfolio Risk Dashboard\n",
    "======================================\n",
    "*One‑file script → prints a readable, line‑by‑line risk sheet.*\n",
    "\n",
    "Key parameters\n",
    "--------------\n",
    "YEARS, CONF_LVL, PORTF_VALUE_USD, WEIGHTS, PROXY.\n",
    "\n",
    "Guarantees\n",
    "----------\n",
    "• Any funded ticker missing data aborts.  \n",
    "• Only pre‑IPO gaps filled (no smoothing halts).  \n",
    "• Final dashboard prints one metric per line.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd, numpy as np, yfinance as yf\n",
    "import logging, contextlib, io, os\n",
    "\n",
    "# ────────── 1) USER SETTINGS ──────────\n",
    "YEARS           = 5\n",
    "CONF_LVL        = 0.95\n",
    "PORTF_VALUE_USD = 100_000\n",
    "RISK_FREE_RATE  = .05\n",
    "\n",
    "# --- Config ---\n",
    "excel_path = r\"E:\\STOCKS\\Portfolio.xlsx\"  # <-- Update this\n",
    "sheet_name = \"Dashboard\"\n",
    "col_ticker, col_weight = \"B\", \"L\"\n",
    "row_start, row_end = 7, 36  # inclusive\n",
    "output_dir = \"csv_export\"\n",
    "output_file = os.path.join(output_dir, \"portfolio_weights.csv\")\n",
    "\n",
    "# === LOAD TICKERS AND WEIGHTS ===\n",
    "nrows = row_end - row_start + 1\n",
    "df = pd.read_excel(\n",
    "    excel_path,\n",
    "    sheet_name=sheet_name,\n",
    "    usecols=f\"{col_ticker},{col_weight}\",\n",
    "    skiprows=row_start - 1,\n",
    "    nrows=nrows,\n",
    "    header=None,\n",
    "    names=[\"Ticker\", \"Weight\"]\n",
    ").dropna()\n",
    "\n",
    "# === CLEANING ===\n",
    "df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip().replace({\"$\": \"SGOV\"})\n",
    "df[\"Weight\"] = pd.to_numeric(df[\"Weight\"], errors=\"coerce\")\n",
    "\n",
    "# === GROUP AND SUM DUPLICATE TICKERS ===\n",
    "df = df.groupby(\"Ticker\", as_index=False)[\"Weight\"].sum()\n",
    "\n",
    "# === BUILD FINAL WEIGHTS DICTIONARY ===\n",
    "WEIGHTS = dict(zip(df[\"Ticker\"], df[\"Weight\"]))\n",
    "total = sum(WEIGHTS.values())\n",
    "\n",
    "# === DEBUG PRINT (Sorted Descending) ===\n",
    "print(\"✅ Parsed portfolio weights:\\n\")\n",
    "for ticker, weight in sorted(WEIGHTS.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{ticker} = {weight * 100:.2f}%')\n",
    "print(f\"\\n✅ Total portfolio weight: {total * 100:.2f}%\")\n",
    "\n",
    "# === VALIDATION ===\n",
    "if not abs(total - 1.0) < 1e-6:\n",
    "    raise ValueError(f\"❌ Portfolio weights sum to {total:.4f}, expected 1.0.\")\n",
    "print(\"✅ Portfolio weights loaded and validated successfully.\")\n",
    "\n",
    "# === EXPORT TO CSV ===\n",
    "\n",
    "# Sort: non-zero weights (descending), then zero weights (alphabetical)\n",
    "nonzero_df = df[df[\"Weight\"] > 0].sort_values(by=\"Weight\", ascending=False)\n",
    "zero_df = df[df[\"Weight\"] == 0].sort_values(by=\"Ticker\")\n",
    "export_df = pd.concat([nonzero_df, zero_df], ignore_index=True)\n",
    "\n",
    "# Write to CSV\n",
    "import os\n",
    "output_dir = \"csv_export\"\n",
    "output_file = os.path.join(output_dir, \"portfolio_weights.csv\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "export_df.to_csv(output_file, index=False)\n",
    "print(f\"Wrote: {output_file}\")\n",
    "\n",
    "BENCH_SPY, BENCH_QQQ = \"SPY\", \"QQQ\"\n",
    "\n",
    "PROXY = {\n",
    "    \"RDDT\": [(\"SNAP\", .4), (\"GOOGL\", .3), (\"PLTR\", .15), (\"HOOD\", .15)],\n",
    "    \"HOOD\": [(\"COIN\", .35), (\"CBOE\", .25), (\"SQ\", .15), (\"MSTR\", .15), (\"PLTR\", .10)],\n",
    "    \"NBIS\": [(\"SMCI\", .35), (\"SNOW\", .25), (\"PLTR\", .2), (\"NET\", .1), (\"PATH\", .1)], \n",
    "    \"SNOW\": [(\"MDB\", .3), (\"DDOG\", .25), (\"NET\", .2), (\"PLTR\", .15), (\"MSFT\", .1)],\n",
    "    \"SHOP\": [(\"SQ\", .35), (\"WIX\", .25), (\"PLTR\", .2), (\"MELI\", .2)],\n",
    "    \"ULTY\": [(\"JPEQ\", 1)]\n",
    "}\n",
    "\n",
    "# ────────── 2) HELPERS ──────────\n",
    "logging.getLogger(\"yfinance\").setLevel(logging.CRITICAL)\n",
    "TRADING_DAYS_YR = 252\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def silence():\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        yield\n",
    "\n",
    "def adj_close(df: pd.DataFrame, t: str) -> pd.Series:\n",
    "    close = df.xs(\"Close\", level=0, axis=1).squeeze(\"columns\") if isinstance(df.columns, pd.MultiIndex) else df[\"Close\"]\n",
    "    close.name = t\n",
    "    return close\n",
    "\n",
    "# ────────── 3) DOWNLOAD PRICES ──────────\n",
    "window = int(YEARS * TRADING_DAYS_YR)\n",
    "start  = pd.Timestamp.today().normalize() - pd.tseries.offsets.BDay(window + 40)\n",
    "end    = pd.Timestamp.today().normalize()\n",
    "\n",
    "proxy_members = {m for v in PROXY.values() for m in ([v] if isinstance(v, str) else [x for x, _ in v])}\n",
    "universe = set(WEIGHTS) | {BENCH_SPY, BENCH_QQQ} | proxy_members\n",
    "\n",
    "prices: dict[str, pd.Series] = {}\n",
    "for tic in universe:\n",
    "    with silence():\n",
    "        raw = yf.download(tic, start=start, end=end, auto_adjust=True, progress=False, threads=False)\n",
    "    if raw.empty:\n",
    "        if WEIGHTS.get(tic, 0) > 0:\n",
    "            raise RuntimeError(f\"Missing data for funded ticker {tic}\")\n",
    "        continue\n",
    "    prices[tic] = adj_close(raw, tic)\n",
    "\n",
    "if BENCH_QQQ not in prices:\n",
    "    raise RuntimeError(\"QQQ data missing\")\n",
    "\n",
    "df = pd.concat(prices.values(), axis=1)\n",
    "qqq_series = df[BENCH_QQQ].ffill()\n",
    "\n",
    "# ────────── 4) PROXY FILL ──────────\n",
    "\n",
    "def proxy_ser(spec):\n",
    "    return df.get(spec, qqq_series) if isinstance(spec, str) else pd.concat([df.get(t, qqq_series)*w for t, w in spec], axis=1).sum(axis=1)\n",
    "\n",
    "filled = []\n",
    "for tic, w in WEIGHTS.items():\n",
    "    base = df.get(tic, pd.Series(index=df.index, dtype=float, name=tic))\n",
    "    prox = proxy_ser(PROXY.get(tic, BENCH_QQQ))\n",
    "    first = base.first_valid_index()\n",
    "    combined = prox if first is None else base.combine_first(prox.where((base.index < first) & base.isna()))\n",
    "    filled.append(combined)\n",
    "\n",
    "filled.extend([df[BENCH_SPY], df[BENCH_QQQ]])\n",
    "prices_clean = pd.concat(filled, axis=1)\n",
    "prices_clean = prices_clean.loc[:, ~prices_clean.columns.duplicated()]  # dedupe\n",
    "prices_clean = prices_clean.dropna().tail(window + 1)\n",
    "\n",
    "# ────────── 5) RETURNS ──────────\n",
    "rets = prices_clean.pct_change().dropna()\n",
    "active_weights = {t: w for t, w in WEIGHTS.items() if t in rets.columns and w > 0}\n",
    "portfolio_r = rets[list(active_weights)] @ pd.Series(active_weights)\n",
    "spy_r = rets[BENCH_SPY]\n",
    "qqq_r = rets[BENCH_QQQ]\n",
    "\n",
    "# ────────── 6) METRICS ──────────\n",
    "ann_ret = (1+portfolio_r).prod()**(TRADING_DAYS_YR/len(portfolio_r))-1\n",
    "ann_vol = portfolio_r.std(ddof=0)*np.sqrt(TRADING_DAYS_YR)\n",
    "sharpe  = np.nan if ann_vol==0 else (ann_ret-RISK_FREE_RATE)/ann_vol\n",
    "cum = (1+portfolio_r).cumprod()\n",
    "max_dd = (cum/cum.cummax()-1).min()\n",
    "alpha = 1-CONF_LVL\n",
    "var_pct = np.percentile(portfolio_r, alpha*100)\n",
    "cvar_pct = portfolio_r[portfolio_r<=var_pct].mean()\n",
    "beta_spy = portfolio_r.cov(spy_r)/spy_r.var()\n",
    "beta_qqq = portfolio_r.cov(qqq_r)/qqq_r.var()\n",
    "\n",
    "# ────────── 7) DASHBOARD ──────────\n",
    "level = int(CONF_LVL*100)\n",
    "print(\"\\n\".join([\n",
    "    f\"Risk Dashboard - {YEARS} years ({len(portfolio_r)} days)\",\n",
    "    f\"Annualized Return      : {ann_ret:.2%}\",\n",
    "    f\"Annualized Volatility  : {ann_vol:.2%}\",\n",
    "    f\"Sharpe Ratio (rf={RISK_FREE_RATE:.0%})   : {sharpe:.2f}\",\n",
    "    f\"Max Drawdown           : {max_dd:.2%}\",\n",
    "    f\"1-Day VaR  ({level}%): {var_pct:.2%} (~${var_pct*PORTF_VALUE_USD:,.0f})\",\n",
    "    f\"1-Day CVaR ({level}%): {cvar_pct:.2%} (~${cvar_pct*PORTF_VALUE_USD:,.0f})\",\n",
    "    f\"Beta vs. S&P 500       : {beta_spy:.2f}\",\n",
    "    f\"Beta vs. Nasdaq-100    : {beta_qqq:.2f}\",\n",
    "    f\"❤️ DAISY ❤️\"\n",
    "]))\n",
    "\n",
    "# ────────── 8) EXPORT RETURNS TO CSV ──────────\n",
    "from pathlib import Path\n",
    "\n",
    "# 1️⃣ Output folder\n",
    "output_dir = Path(\"csv_export\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 2️⃣ Main \"Returns.csv\": portfolio tickers, proxy-adjusted\n",
    "returns_all = pd.concat(filled, axis=1)\n",
    "returns_all = returns_all.loc[:, ~returns_all.columns.duplicated()]  # Remove duplicated tickers (e.g. QQQ)\n",
    "returns_all = returns_all.dropna().pct_change().dropna()\n",
    "returns_all.index.name = \"Date\"\n",
    "returns_all.to_csv(output_dir / \"Returns.csv\")\n",
    "print(f\"✅ Wrote: {output_dir/'Returns.csv'}\")\n",
    "\n",
    "# 3️⃣ Per-proxy raw constituent return files\n",
    "for target, spec in PROXY.items():\n",
    "    if isinstance(spec, str):\n",
    "        if spec in df:\n",
    "            returns_proxy = df[[spec]].pct_change().dropna()\n",
    "            returns_proxy.index.name = \"Date\"\n",
    "            returns_proxy.to_csv(output_dir / f\"{target}.csv\")\n",
    "            print(f\"✅ Wrote: {output_dir / f'{target}.csv'}\")\n",
    "    else:\n",
    "        tickers = [t for t, _ in spec if t in df.columns]\n",
    "        if tickers:\n",
    "            returns_proxy = df[tickers].pct_change().dropna()\n",
    "            returns_proxy.index.name = \"Date\"\n",
    "            returns_proxy.to_csv(output_dir / f\"{target}.csv\")\n",
    "            print(f\"✅ Wrote: {output_dir / f'{target}.csv'}\")\n",
    "\n",
    "print(f\"\\n📁 All exports saved in: {output_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd97875-1fd8-44d3-ac1c-5a6bce993e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86253a38-7583-49fb-a1db-b02cb078e8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
