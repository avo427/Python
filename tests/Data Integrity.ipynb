{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bba5059-4006-45bb-ac4c-461eb475af22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 unique tickers (including QQQ)\n",
      "Downloaded ASTS, rows: 501\n",
      "Downloaded BRK/B, rows: 501\n",
      "Downloaded CBOE, rows: 501\n",
      "Downloaded COIN, rows: 501\n",
      "Downloaded DDOG, rows: 501\n",
      "Downloaded GOOGL, rows: 501\n",
      "Downloaded HOOD, rows: 501\n",
      "Downloaded INTU, rows: 501\n",
      "Downloaded JEPQ, rows: 501\n",
      "Downloaded MDB, rows: 501\n",
      "Downloaded MELI, rows: 501\n",
      "Downloaded META, rows: 501\n",
      "Downloaded MSFT, rows: 501\n",
      "Downloaded MSTR, rows: 501\n",
      "Downloaded NBIS, rows: 179\n",
      "Downloaded NET, rows: 501\n",
      "Downloaded PATH, rows: 501\n",
      "Downloaded PLTR, rows: 501\n",
      "Downloaded QQQ, rows: 501\n",
      "Downloaded QQQM, rows: 501\n",
      "Downloaded RDDT, rows: 326\n",
      "Downloaded SGOV, rows: 501\n",
      "Downloaded SHOP, rows: 501\n",
      "Downloaded SMCI, rows: 501\n",
      "Downloaded SNAP, rows: 501\n",
      "Downloaded SNOW, rows: 501\n",
      "Downloaded TQQQ, rows: 501\n",
      "Downloaded ULTY, rows: 341\n",
      "Downloaded WIX, rows: 501\n",
      "Downloaded XYZ, rows: 501\n",
      "\n",
      "Saved price data to: ../data/yahoo_prices.csv\n",
      "Data quality check complete. Results saved to: yahoo_quality_check.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# ────────── CONFIG ──────────\n",
    "PORTFOLIO_PATH = \"../data/portfolio_weights.csv\"\n",
    "PROXY_PATH = \"../data/proxy_mapping.csv\"\n",
    "OUTPUT_PATH = \"../data/yahoo_prices.csv\"\n",
    "YEARS = 2\n",
    "MIN_DAYS_REQUIRED = 60\n",
    "STALE_DAYS_THRESHOLD = 5\n",
    "VOL_LOWER_BOUND = 0.001\n",
    "VOL_UPPER_BOUND = 0.20\n",
    "\n",
    "# ────────── NORMALIZATION ──────────\n",
    "def normalize_for_yahoo(ticker: str) -> str:\n",
    "    return ticker.strip().replace(\"/\", \"-\").upper()\n",
    "\n",
    "# ────────── LOAD TICKERS ──────────\n",
    "portfolio_df = pd.read_csv(PORTFOLIO_PATH)\n",
    "portfolio_tickers = set(portfolio_df['Ticker'].dropna().unique())\n",
    "\n",
    "try:\n",
    "    proxy_df = pd.read_csv(PROXY_PATH)\n",
    "    proxy_tickers = set(proxy_df['Proxy'].dropna().unique())\n",
    "except FileNotFoundError:\n",
    "    proxy_tickers = set()\n",
    "\n",
    "original_tickers = sorted(portfolio_tickers | proxy_tickers | {\"QQQ\"})\n",
    "print(f\"Found {len(original_tickers)} unique tickers (including QQQ)\")\n",
    "\n",
    "# ────────── DATE RANGE ──────────\n",
    "start_date = (datetime.today() - timedelta(days=365 * YEARS)).strftime(\"%Y-%m-%d\")\n",
    "end_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ────────── DOWNLOAD DATA ──────────\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "downloaded_prices = {}\n",
    "failed = []\n",
    "\n",
    "for original in original_tickers:\n",
    "    yf_ticker = normalize_for_yahoo(original)\n",
    "    try:\n",
    "        df = yf.download(yf_ticker, start=start_date, end=end_date, progress=False)\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Empty DataFrame\")\n",
    "\n",
    "        # Handle MultiIndex\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            if \"Close\" in df.columns.get_level_values(0):\n",
    "                close_df = df[\"Close\"]\n",
    "                if isinstance(close_df, pd.DataFrame) and close_df.shape[1] == 1:\n",
    "                    df = close_df.iloc[:, 0]\n",
    "                elif isinstance(close_df, pd.Series):\n",
    "                    df = close_df\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected 'Close' structure in MultiIndex\")\n",
    "            else:\n",
    "                raise ValueError(\"No 'Close' level found in MultiIndex\")\n",
    "        elif \"Close\" in df.columns:\n",
    "            df = df[\"Close\"]\n",
    "        else:\n",
    "            raise ValueError(\"No 'Close' column found\")\n",
    "\n",
    "        if not isinstance(df, pd.Series):\n",
    "            raise ValueError(\"Extracted data is not a Series\")\n",
    "\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df.name = original\n",
    "        downloaded_prices[original] = df\n",
    "        print(f\"Downloaded {original}, rows: {len(df)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append(original)\n",
    "        print(f\"Failed to get {original}: {e}\")\n",
    "\n",
    "# ────────── FORMAT & SAVE ──────────\n",
    "if downloaded_prices:\n",
    "    if \"QQQ\" not in downloaded_prices:\n",
    "        raise ValueError(\"QQQ failed to download. Cannot proceed without reference calendar.\")\n",
    "\n",
    "    qqq_series = downloaded_prices[\"QQQ\"].dropna()\n",
    "    qqq_calendar = qqq_series.index\n",
    "\n",
    "    for k in downloaded_prices:\n",
    "        reindexed = downloaded_prices[k].reindex(qqq_calendar)\n",
    "        if reindexed.isnull().all():\n",
    "            print(f\"Warning: All NaNs after reindex for {k}. Check ticker history.\")\n",
    "        downloaded_prices[k] = reindexed\n",
    "\n",
    "    combined_df = pd.DataFrame(downloaded_prices)\n",
    "    combined_df.index.name = \"Date\"\n",
    "    combined_df = combined_df.sort_index()\n",
    "\n",
    "    # Move QQQ to second column (after Date)\n",
    "    columns = combined_df.columns.tolist()\n",
    "    columns.remove(\"QQQ\")\n",
    "    columns = [\"QQQ\"] + sorted(columns)\n",
    "    combined_df = combined_df[columns]\n",
    "\n",
    "    combined_df.to_csv(OUTPUT_PATH, index=True, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved price data to: {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"\\nNo valid time series data to write.\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nFailed to retrieve data for {len(failed)} tickers:\")\n",
    "    for t in failed:\n",
    "        print(f\" - {t}\")\n",
    "\n",
    "# ────────── QA CHECK ──────────\n",
    "df = pd.read_csv(OUTPUT_PATH, parse_dates=[\"Date\"])\n",
    "df = df.set_index(\"Date\").sort_index()\n",
    "tickers = df.columns.tolist()\n",
    "\n",
    "# Use QQQ as reference calendar\n",
    "if \"QQQ\" not in df.columns:\n",
    "    raise ValueError(\"QQQ not found in final price data. Required for calendar alignment.\")\n",
    "\n",
    "qqq = df[\"QQQ\"].dropna()\n",
    "qqq_calendar = qqq.index\n",
    "\n",
    "results = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    record = {\"Ticker\": ticker}\n",
    "    price_data = df[ticker]\n",
    "    valid_series = price_data.dropna()\n",
    "\n",
    "    # 1. Existence & Coverage\n",
    "    if valid_series.empty:\n",
    "        record[\"Existence & Coverage\"] = \"FAIL: No data\"\n",
    "    elif len(valid_series) < MIN_DAYS_REQUIRED:\n",
    "        record[\"Existence & Coverage\"] = f\"WARN: Only {len(valid_series)} days\"\n",
    "    elif (datetime.today() - valid_series.index[-1]).days > STALE_DAYS_THRESHOLD:\n",
    "        record[\"Existence & Coverage\"] = f\"WARN: Stale ({valid_series.index[-1].date()})\"\n",
    "    else:\n",
    "        record[\"Existence & Coverage\"] = \"OK\"\n",
    "\n",
    "    # 2. Calendar Alignment vs QQQ\n",
    "    if not valid_series.index.is_monotonic_increasing:\n",
    "        record[\"Calendar Alignment\"] = \"FAIL: Unsorted dates\"\n",
    "    elif valid_series.index.has_duplicates:\n",
    "        record[\"Calendar Alignment\"] = \"FAIL: Duplicate dates\"\n",
    "    elif not valid_series.index.equals(qqq_calendar):\n",
    "        record[\"Calendar Alignment\"] = f\"WARN: Missing {len(set(qqq_calendar) - set(valid_series.index))} days\"\n",
    "    else:\n",
    "        record[\"Calendar Alignment\"] = \"OK\"\n",
    "\n",
    "    # 3. Raw Price Sanity (Relaxed Inf/NaN if Calendar already WARNed)\n",
    "    if (price_data <= 0).any():\n",
    "        record[\"Raw Price Sanity\"] = \"FAIL: Non-positive price\"\n",
    "    elif price_data.rolling(10).apply(lambda x: x.nunique() == 1).any():\n",
    "        record[\"Raw Price Sanity\"] = \"WARN: Flatline >=10d\"\n",
    "    elif not np.isfinite(price_data).all():\n",
    "        if record[\"Calendar Alignment\"].startswith(\"WARN:\"):\n",
    "            record[\"Raw Price Sanity\"] = \"OK (skipped: short history)\"\n",
    "        else:\n",
    "            record[\"Raw Price Sanity\"] = \"FAIL: Inf/NaN\"\n",
    "    elif price_data.pct_change(fill_method=None).abs().gt(1).any():\n",
    "        record[\"Raw Price Sanity\"] = \"WARN: Absurd jump\"\n",
    "    else:\n",
    "        record[\"Raw Price Sanity\"] = \"OK\"\n",
    "\n",
    "    # 4. Return Sanity\n",
    "    returns = price_data.pct_change(fill_method=None).dropna()\n",
    "    if returns.empty:\n",
    "        record[\"Return Sanity\"] = \"FAIL: No returns\"\n",
    "    elif returns.abs().gt(0.5).any():\n",
    "        record[\"Return Sanity\"] = \"WARN: Extreme return\"\n",
    "    elif returns.rolling(21).std().gt(VOL_UPPER_BOUND).any():\n",
    "        record[\"Return Sanity\"] = \"WARN: High vol\"\n",
    "    elif returns.rolling(21).std().lt(VOL_LOWER_BOUND).all():\n",
    "        record[\"Return Sanity\"] = \"WARN: Near-zero vol\"\n",
    "    elif returns.isnull().any():\n",
    "        record[\"Return Sanity\"] = \"FAIL: NaNs\"\n",
    "    else:\n",
    "        record[\"Return Sanity\"] = \"OK\"\n",
    "\n",
    "    results.append(record)\n",
    "\n",
    "qa_df = pd.DataFrame(results)\n",
    "required_columns = [\n",
    "    \"Ticker\", \n",
    "    \"Existence & Coverage\", \n",
    "    \"Calendar Alignment\", \n",
    "    \"Raw Price Sanity\", \n",
    "    \"Return Sanity\"\n",
    "]\n",
    "qa_df = qa_df[[col for col in required_columns if col in qa_df.columns]]\n",
    "qa_df.sort_values(\"Ticker\", inplace=True)\n",
    "qa_df.to_csv(\"../data/yahoo_quality_check.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Data quality check complete. Results saved to: yahoo_quality_check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b9445-463e-4b43-bbd3-8760066966e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff3086-e154-45ad-b3db-334c99aa6b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
